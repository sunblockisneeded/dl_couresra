정규화와 train을 빠르게 만들 수 있는 여러 기술
DL의 일반적인 레시피
train이 잘 됩니까? ----아니오---> 1.더 큰 네트워크, 2.더 긴 train(더 많은 iteration)  3.다른 아키텍쳐 시도
↓
test가 잘 됩니까?  ----아니오---> 1.더 많은 dataset으로 훈련 2. 정규화 3. 다른 아키텍쳐
↓
done

정규화 : 정규- 규제한다
L2정규화
1. cost + (람다/2m * ||w||^2) , 2.dw = 기존 dw + 람다/m * W 3.람다 10-2~10-5 or 0.01~1
->w를 줄이는 방향으로 학습하게 됨

cost function에 일정한 값을 추가해서 과적합되는것을 방지하는 기법
2L reg -> cost + (람다/2m * ||w||^2)을 더해줌 -> ||w||^2 프로베니우스 노름
프로베니우스 노름 : 각 W의 요소들 제곱의 합
cost = j(w,b) + (람다/2m * ||w||^2) 인데 minimize(cost)이니 w가 줄어드는 방향으로 학습
특정 w가 큰 경우를 더 잘 죽여버리는 경향이 있음.
특정 w가 크가 -> 특정 데이터셋에서의 경로가 더 영향을 크게 미친다 -> 특정 데이터셋에 과적합
-> 특정 데이터셋에 대한 w를 줄여버리면서, 과적합을 죽여버리는 것
람다가 너무 크면 cost를 압도해버릴 여지가 있는데 정규화가 되는 람다의 적절한 수준이 필요
-> 람다항을 개별 w에 대한 의존도를 줄이거나 특정 데이터셋에 과적합되지 않을 정도로 만드는 람다가 필요 -> 10-5~ 10-2정도까지를 로그스케일로 탐색
로지스틱 reg : 0.01~1   딥러닝: 각 w의 영향이 더 작을 것임으로 10-5 ~ 10-3정도 

2.Dropout 정규화 
1. 드랍아웃 벡터(마스킹 벡터)로 마스킹하는 기법
2. d = np.random.rand(a5.shape[0],a5.shape[1]) < dropout_ratio
3. a = a*d 
4. a = a / keep_prob (기댓값 보정)
5. 테스트할 때는 dropout적용하지 않음
6. 층별로 keep_prob다르게 할수도
->컴퓨터 비전에는 거의 기본값
단점 : 비용함수가 잘 정의되지 않음

정규화라는 이름이 붙는 이유: 특정 경로나 특정 param을 규제하기 때문에
0~1까지 랜덤으로 난수 생성 후, 0.5보다 크면 False를 덮어씌우는 식으로 마스크를 만듬
-> 각 층별로 적용
예를 들어 l=5에 dropout를 drop_ratio = 0.3으로 한다?
np.random.rand(a5.shape[0],a5.shape[1]) < 0.3
30%는 False, 70%는 True를 씌우는 마스크 생성
그 후 a5 * d*5를 하면 dropout된 a_d_5 완성

dropout_ratio = 30% , keep_prob = 0.7이라면 
테스트에서 drioout을 끄니까 더 과도하게 나올 것
원래는 1이 나와야하는데 0.7이 나와버림
따라서  train 단계에서 , dropout을 적용 한 후에 
a5 = a5 / 0.7 이렇게 값을 늘려줘야 기댓값이 맞음


3.overfitting 줄이는 테크닉
1.data augmentation ->사진 뒤집고 비틀고 찌그러뜨리면서 변형을 주기
-완전히 새로운 정보를 추가하는것보다는 도움이 되지는 않지만 

2.early_stopping ->가끔 쓸 것
test loss가 train_loss와 다르게 어느수준에서 증가한다면, 그 iteration에서 멈추기
직교화 : min J(좋은 train)과 not overfit(과적합 방지)는, 어느정도 독립된 요소로 컨트롤됨
min J ---> 더 큰 모델, 더 많은 iteration
not overfit ---> 더 많은 데이터, 정규화
early stopping은 이 두 요소를 동시에 건드려버림. -> min J와 overfit문제를 동시에 건들수가 있음


4.기타  

1. train 속도 늘리기 : 입력 정규화(regulation이 아니라 normalize)
   1.  x = x- u /  σ.  2. test data를 조정할 때도 같은 u와  σ을 사용할 것 
여기서 속도 늘리기는 iteration이 더 적게 필요하단 뜻임 
평균을 0, 분산을 1로 만들어보기 ->(막 평균이 230 분산이 막 푸아아악 이런 것에서말이죠) 평균이 막 1.6 이런거는 괜찮아. 처리 안해도 잘 작동해

2.파라미터 초기화에 대하여
np.random.randn * sqrt(1/입력 레이어의 히든유닛 수) 
/relu사용시에는 sqrt(2/입력 레이어의 히든유닛/ 수)
tahn에는 자비에르 초기화 (입력,다음레이어의 히든유닛수의 평균을 사용)

W = np.random.randn(next_lyer, before_layer) * 0.01 
 -> 0.01 대신에 hidden 유닛의 루트
만약 W[3]의 히든유닛이 100개라면  루트(1/100) -> 1/ 10
   -> np.random.randn() * 0.1
ReLU를 쓸 때는 루트(1/100)이 아니라 (2/100)으로 분산을  1/n 이 아니라 2/n으로 설정

tanh에서는 xavier(자비에르 초기화)
전 레이어와 이후 레이어의 평균을 사용
입력 레이어 히든유닛이 20, 다음 레이어의 히든유닛이 40이라면, 루트(1/30)을 곱해줌

5.grad check
디버깅에서만!
-정규화 항에 신경쓰기
-dropout 끄기

실제 우리가 사용할 dw가 맞는지 어떻게 확신할까요?
우리가 사용할 dw 와 실제 비용함수에서  k와 k+epsilon -> 의 변화를 관찰해
dw와 k-(k+epsilon)의 변화가 대략적으로 일치한다면 dw가 맞는게 아닐까 라는 컨셉
10-7 good! 10-5 흠  10-3 뭔가 잘못됐다




