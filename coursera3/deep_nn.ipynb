{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 L-layer NN\n",
    "\n",
    "hidden lyaer가 없는건, 판별선이 선형으로만 이뤄져 한계가 있었고\n",
    "hidden lyaer가 하나 이상 있는것 또한 복잡한 패턴을 인식하는데에 있어서 쉽지 않았음.\n",
    "따라서 이 hidden layer를 2개 혹은 n개로 늘릴 것임\n",
    "이때 당연히 hidden layer units은 하이퍼파라미터로 설정하게 될 것. \n",
    "\n",
    "w1 = ~~ W2 = ~ 했던 식보다 좀 더 현명한 방법을 쓰게 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 전에 n_x ,n_h, n_y 를 정했다면 이번에는 레이어가 몇개가 될지, \n",
    "# 그리고 그 hidden unit이 어떻게 될지 결정해야할 것\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    '''\n",
    "    layer_dims = [n_x, n_h1, n_h2, ...,n_y]\n",
    "    지난번에 배운대로 대칭+기울기소실 문제를 해결하기 위해서 w는 \n",
    "    randn * 0.01을, b는 0으로 초기화\n",
    "    '''\n",
    "    \n",
    "    # W1 W2 b2 등을 저장할 params라는 dictionary\n",
    "    params = {} \n",
    "\n",
    "    # hidden layer가 1개인 상황이었다면\n",
    "    # layer_dims = {n_x, n_h, n_y}\n",
    "    # params에는 {w1 b1과 w2 b2가 필요할 것}\n",
    "    # 따라서 len (layer_dims) - 1번만큼 실행되어야 함.\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L): #{W1 : ...]} 과 같이 저장되게 됨\n",
    "        #w의 차원 수는 (after_dim, before_dim)과 같은 식으로 설정\n",
    "        params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        params['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert(params['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(params['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.00715987, -0.01001779, -0.01792443],\n",
       "        [ 0.00630844, -0.00069599,  0.00053803],\n",
       "        [-0.00059532, -0.00707596, -0.00427427],\n",
       "        [ 0.00366613,  0.00270344,  0.0031276 ]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.00307764, -0.01478633, -0.00290341, -0.00375866],\n",
       "        [ 0.01017189,  0.01955388, -0.00550092,  0.00886382],\n",
       "        [-0.01233847,  0.00175461,  0.01416491, -0.00815376],\n",
       "        [ 0.00112716,  0.00583276, -0.01887958,  0.00100959],\n",
       "        [ 0.00286092, -0.002107  , -0.01719883,  0.00756517]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters_deep([3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    '''\n",
    "    np.maximum\n",
    "    a = (1,3,5)\n",
    "    b = (6,4,2)\n",
    "    np.maximum-> (6,4,5) 그 자리에서 높은 값 채워 넣음\n",
    "    ReLU의 의도와 알맞는 친구다 \n",
    "    '''\n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A_prev, W, b, activation = \"relu\"):\n",
    "    '''\n",
    "    returns: A[l] , cache\n",
    "    cache = (linear_cache , activation_cache)\n",
    "    linear_cache = A[l-1], W[l], B[l]\n",
    "    activation_cache = Z[l]\n",
    "    '''\n",
    "\n",
    "    Z = W @ A_prev + b\n",
    "    linear_cache = (A_prev, W, b)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01474844],\n",
       "       [0.00284574],\n",
       "       [0.00241597],\n",
       "       [0.        ],\n",
       "       [0.0151312 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_prev = np.random.randn(4,1)\n",
    "W = np.random.randn(5,4) * 0.01\n",
    "b = np.zeros((5,1))\n",
    "A, _ = forward(A_prev, W, b)\n",
    "display(A)\n",
    "del A_prev, W ,b, A, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "L번 forward 하기\n",
    "'''\n",
    "def L_model_forward(X, params):\n",
    "    '''\n",
    "    X와 파라미터를 넣어주면, L번 순방향 전파를 해서 Y_hat 값을 출력\n",
    "    return: y_hat, caches\n",
    "    caches = linear_caches, activation_caches\n",
    "    linear_caches:      (A[l-1], W[l], b[l])들\n",
    "    activation_caches : (Z[l])들\n",
    "    '''\n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    # params = {W1: ...  , b1... , ..., WL: ..., bL: ... } 이기 때문에 \n",
    "    # params의 len 은 2L임 (L은 레이어의 갯수)\n",
    "    # 나누기는 값을 float로 반환하기 때문에 정수를 반환하는 // 몫을 사용\n",
    "    # L = int(len(params) / 2)\n",
    "    L = len(params) //2 \n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward(A_prev,\n",
    "                           params[f'W{str(l)}'],\n",
    "                           params[f'b{str(l)}'],\n",
    "                           activation = \"relu\",\n",
    "                           )\n",
    "        caches.append(cache)\n",
    "    # L-1번째까지 forward가 작동한 상황\n",
    "    y_hat, cache = forward(A ,\n",
    "                           params[\"WL\"],\n",
    "                           params[\"bL\"],\n",
    "                           activation = \"sigmoid\"\n",
    "                           )\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return y_hat, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실 계산하기\n",
    "def compute_cost(y_hat, Y):\n",
    "    '''\n",
    "    m개의 데이터셋에서 도출해낸 cost\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    cost = np.sum(Y* np.log(y_hat) + (1-Y) * np.log(1-y_hat)) / -m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "forward  (A[l-1]) -- W,b ------> (Z[l]) -- activation --> A[l]  \n",
    "backward (A[l-1]) <-- W,b ------ (Z[l]) -- activation <-- A[l],Y  \n",
    "grad:  dA[l-1]<-- DW[l],Db[l] <-- DZ[l] <--------------- A[l], Y\n",
    "과 같은 순서를 생각해볼 수 있음\n",
    "backward는 dA[l-1]<-- DW[l],Db[l] <-- DZ[l] 의 과정을 하는 것\n",
    "'''\n",
    "def backward(dZ, cache):\n",
    "    '''\n",
    "    forward처럼, 기본적인 backward 정의\n",
    "    forward가  Z = W@A_prev + B 에서\n",
    "    input : W, A_prev, b   output : Z 라면 \n",
    "    backward는 \n",
    "    input : Z              output : W, A_prev, b \n",
    "    이지 않을까\n",
    "    미리 계산된 식을 사용한다\n",
    "    '''\n",
    "    A_prev, W,b = cache # linear cache겠죵?\n",
    "\n",
    "    #A_prev에서는 데이터의 수 m만큼 열이 있겠죵? \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (dZ @ A_prev.T)/m\n",
    "    db = np.sum(dZ, axis= 1, keepdims= True)/m\n",
    "    dA_prev = W.T@dZ\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    '''\n",
    "    cache = Z\n",
    "    '''\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z < 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    '''\n",
    "    cache = Z\n",
    "    '''\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "forward  (A[l-1]) -- W,b ------> (Z[l]) -- activation --> A[l]  \n",
    "backward (A[l-1]) <-- W,b ------ (Z[l]) -- activation <-- A[l],Y  \n",
    "grad:  dA[l-1]<-- DW[l],Db[l] <-- DZ[l] <--------------- A[l], Y\n",
    "과 같은 순서를 생각해볼 수 있음\n",
    "backward는 dA[l-1]<-- DW[l],Db[l] <-- DZ[l] 의 과정을 했다면\n",
    "backward_with_activation은  DZ[l] <--------------- A[l], Y 의 과정을 하는 것\n",
    "'''\n",
    "def backward_with_activation(dA, cache, activation):\n",
    "    '''\n",
    "    cache = Z\n",
    "    dZ = dA * ACTIVATION(Z)\n",
    "    return : dA[l-1], dW[l], dB[l]\n",
    "    근데 dA는 뭐 하늘에서 떨어지나요.. 그러게요\n",
    "    최초의 dAL은 손실함수에 의해 계산되고 재귀적으로 dA[L-1]부터 dA[l]로 적용된답니다!\n",
    "\n",
    "    '''\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, dB = backward(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, dB = backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[4 3 2 1]\n",
      "np.devide(x1,x2)는 x1을 x2로 나눕니다\n",
      "위엣 식을 아래로 나눴다고 보면 되겠네요\n",
      "0.25 = 1/4\n",
      "0.66666667 = 2/3\n",
      "1.5 = 3/2\n",
      "4 = 4/1\n",
      "[0.25       0.66666667 1.5        4.        ]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([1,2,3,4])\n",
    "x2 = np.array([4,3,2,1])\n",
    "print(x1)\n",
    "print(x2)\n",
    "#print(np.divide(x1,x2))\n",
    "print(\"\"\"np.devide(x1,x2)는 x1을 x2로 나눕니다\n",
    "그냥 나누기랑 똑같아요\n",
    "근데 0으로 나누거나 하는 상황에서 처리 방식을 지정할 수 있음\n",
    "result = np.divide([1,0,3] , [0,0,3] ,out = np.zeros_like(a), where=b!=0) -> b가 0이 아닐때만 나눗셈을 수행하겠다\n",
    "\"\"\")\n",
    "print(x1/x2)\n",
    "del x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "forward  (A[l-1]) -- W,b ------> (Z[l]) -- activation --> A[l]  \n",
    "backward (A[l-1]) <-- W,b ------ (Z[l]) -- activation <-- A[l],Y  \n",
    "grad:  dA[l-1]<-- DW[l],Db[l] <-- DZ[l] <--------------- A[l], Y\n",
    "그동한 계산한 caches들과 yhat,y만 있으면 backward할 수 있겠군!\n",
    "최초의 dAL = -( (Y/Y_hat) - (1-Y) / (1-Y_hat) )\n",
    "'''\n",
    "\n",
    "def L_model_backward(Y_hat, Y, caches):\n",
    "    '''\n",
    "    input 중에 \n",
    "    caches = [cache-0, cache-1, ... ,cache-2,... ,cache-L-1] -> L개\n",
    "                  ↓\n",
    "              cache-l = (linear_cache, activation_cache)\n",
    "\n",
    "    output: grads {\n",
    "    \"dA1\" : ...   , \"dW1\" : ... , \"db1\" : ... \n",
    "    \"dA2\" : ...   , \"dW2\" : ... , \"db2\" : ... \n",
    "    .....\n",
    "    \"dAL\" : ...   , \"dWL\" : ... , \"dbL\" : ... }\n",
    "\n",
    "    dZ는요? 걔는 dw da db를 계산하기 위한 라면받침일 뿐입니다\n",
    "    '''\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = Y_hat.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "\n",
    "    # dAL = - ( Y/Y_hat   -   (1-Y) / (1-Y_hat) ) -> y와 y_hat 을 통해 계산가능\n",
    "    dAL = - (np.divide(Y,Y_hat) - np.divide((1-Y),(1-Y_hat)))\n",
    "\n",
    "    #최초의 backward\n",
    "    current_cache = caches[-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = backward_with_activation(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    grads[\"dA\"+str(L-1)] = dA_prev_temp \n",
    "    grads[\"dW\"+str(L)] = dW_temp\n",
    "    grads[\"db\"+str(L)] = db_temp\n",
    "\n",
    "    #L-1번째 LAYER부터 L번째 레이어까지\n",
    "    for l in reversed(range(L-1)):\n",
    "        #l = L-2 L-3 ,..., 2, 1, 0\n",
    "        #l번째에는 relu가 쓰임\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_with_activation(dAL, current_cache, activation = \"relu\")\n",
    "        grads[\"dA\"+str(l)] = dA_prev_temp #dA는 dA0 ~ dAL-1 + dAL\n",
    "        grads[\"dW\"+str(l+1)] = dW_temp #dW1 ~ dWL\n",
    "        grads[\"db\"+str(l+1)] = db_temp #db1 ~ dbL\n",
    "    \n",
    "    return grads\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, lr = 0.03):\n",
    "    '''\n",
    "    W = W - lr * dW    \n",
    "    b = b - lr * db\n",
    "    w,b는 params에, dw와 db는 grads에 있음\n",
    "    params = {W1 : ... , b1 : ... , ~ , WL : ... , BL : ...} \n",
    "    grads  = {dA0: ... , dW1: ... , db1: ... , ~ , dAL: ..., dWL : ..., dWb : ...}\n",
    "    '''\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(params) // 2 #float가 아닌 int를 반환하기 위해 //사용\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        # 위 params과 grads를 통해 업데이트를 하기 위해 다음과 같이 활용\n",
    "        parameters['W' + str(l)] = parameters['W' + str(l)] - lr * grads['dW'+str(l)]\n",
    "        parameters['b' + str(l)] = parameters['b' + str(l)] - lr * grads['db'+str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
