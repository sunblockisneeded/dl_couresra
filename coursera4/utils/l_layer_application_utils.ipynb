{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\"\n",
    "    layer_dims = [n_x, n_h, ... ,n_y]\n",
    "    ex : [12288, 20, 20, ... , 20, 1]\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        params[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        params[\"b\" + str(l)] = np.zeros((layer_dims[l],1))\n",
    "    return params\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_with_activation(a_prev, w, b, activation = \"relu\"):\n",
    "    z = w@a_prev + b\n",
    "\n",
    "    activation_cache = z\n",
    "    linear_cache = (a_prev, w, b)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A = relu(z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(z)\n",
    "    return A, cache\n",
    "\n",
    "    \n",
    "    \n",
    "def forward(X, params):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) //2\n",
    "\n",
    "    for l in range(1, L):\n",
    "        a_prev = A\n",
    "        A, cache = forward_with_activation(a_prev, params[\"W\" + str(l)], params[\"b\" + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    AL , cache = forward_with_activation(A, params[\"W\" + str(L)], params[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches \n",
    "    \n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = np.sum(Y * np.log(AL) + (1 -Y)* np.log(1-AL)) / -m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    '''\n",
    "    activation_cache = z\n",
    "    '''\n",
    "    z = activation_cache\n",
    "    dz = np.array(dA, copy=True)\n",
    "\n",
    "    dz[z<0] = 0\n",
    "    return dz\n",
    "\n",
    "def sigmoid_backward(dA, activation_cache):\n",
    "    '''\n",
    "    activation_cahce = z\n",
    "    '''\n",
    "    z = activation_cache\n",
    "    s = 1 / (1+ np.exp(-z))\n",
    "    dz = dA * s * (1-s)\n",
    "    return dz \n",
    "\n",
    "\n",
    "def backward(dz, linear_cache):\n",
    "    a_prev, W, b = linear_cache\n",
    "    m = a_prev.shape[1]\n",
    "    dW = (dz @ a_prev.T) / m\n",
    "    db = np.sum(dz, axis = 1, keepdims = True)/ m\n",
    "    dA_prev = W.T @ dz\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def backward_with_activation(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dz = relu_backward(dA,activation_cache) # activation_cache = z\n",
    "        dA_prev, dW, db = backward(dz,linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dz = sigmoid_backward(dA, activation_cache) # activation_cache = z\n",
    "        dA_prev, dW, db = backward(dz, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "    \n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    #AL부터 시작하는 backward\n",
    "    dAL = - (np.divide(Y,AL) - np.divide((1-Y),(1-AL)))\n",
    "\n",
    "    current_cache = caches[-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = backward_with_activation(dAL, current_cache, activation=\"sigmoid\")\n",
    "\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_with_activation(dAL, current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "    \n",
    "def update_params(params, grads, lr= 0.03):\n",
    "    parameters = copy.deepcopy(params)\n",
    "\n",
    "    L = len(params) // 2\n",
    "\n",
    "    # w = w - lr * wb\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - lr * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - lr * grads[\"db\" + str(l)]\n",
    "    return parameters\n",
    "\n",
    "    \n",
    "def plot_costs(costs, lr):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(lr))\n",
    "    plt.show()\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
